\documentclass[noindent, nochapname]{tudexercise}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{rotating}
\usepackage{verbatim}
\title{Natural Language Processing and the Web}
\subtitle{Machine Learning Project - Documentation}
\subsubtitle{Benedikt Lins (1799381) und Stefan Thaut (1800351)\\
			Department 20 - Computer Science\\
			\today}
\setlength{\parindent}{0em} 
\begin{document}
	\lstset{language=Java}
	\maketitle
	
	\section{Foundations}
		In this Machine Learning Project we want to develop a named entity recognizer based on a machine learning approach. A named entity is a set of tokens, which form a name. Examples are "New York" or "Angela Merkel". We use a Conditional Random Field as the machine learning model.\\
		Our training set contains a set of tokens with a chunk-annotation and the associated named entity class relating to the IOB-notation. We also have a german training set, that additionally contains a lemma for the given token. We also have a dev set, on which we evaluate our learned model.\\
		An analysis for the english training set shows, that about 83 \% of all tokens are no named entities, as we can see in figure \ref{fig:class_distribution}.
		
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.6]{img/class_distribution.png}
			\caption{Class distribution of the english training set}
			\label{fig:class_distribution}
		\end{figure}
		
		Our CRF-model is based on the CRF-model we heard about in the fifth exercise of the lecture. So we already used the provided features:
		\begin{itemize}
			\item Is the first letter of a token capitalized? (Cap)
			
			\item Number of characters (NoOfChar)
		\end{itemize}
		
		In table 1 we can see the improvement of the measurements with this features in comparison to the baseline. The Micro F1 is increased by about just 0.035, but the Macro F1 is increased by about 0.15.
		
		\subsection{Evaluation} \label{sec:evaluation}
			To have a baseline for an evaluation, with which we can compare our results, we added just a dummy feature to the CRF-model and did a testrun with the english trainingset and the english validationset as testset. The evaluationmetrics are shown in table 1.\\
			To know which combination of features works best, we performed a grid search, to identify the features, that give an improvement in comparison to the baseline. The results are also shown in table 1 in appendix \ref{app:results}.\\
			In a first approach, we used every feature, that gives a positive improvement of the Micro and Macro F1. The evaluation of this setting results in an improvement of 0.00698 of the Micro F1 and 0.03180 of the Macro F1 in comparison to the baseline. For our interest, we added also the features, that give a negative improvement of the measures except the features belonging to the chunks. And remarkably this setting produces again a better result, that is in total a Micro F1 of 0.87529 and a Macro F1 of 0.32658.
	
	\section{Additional features}
		In this section, we describe additional features, with which we have tried to increase the measures.
		
		\subsection{Last two characters of a token}
			At first we have tried to use the last two characters (ltc) of a token as a feature. For the meaningfulness of this feature, we analysed the most frequent ltc for the class \textit{O} and for the tokens, that are not in the class \textit{O}. The ten most frequent ltc are presented in figure \ref{fig:ten_most_frequent_ltc}.
			
			\begin{figure}[h]
				\centering
				\includegraphics[scale=0.5]{img/last_two_letters_per_class.png}
				\caption{This figures show the ten most frequent ltc. The right side shows the ltc for the class \textit{O} and the left side shows the ltc for all tokens, that are not in class \textit{O}}
				\label{fig:ten_most_frequent_ltc}
			\end{figure}
			
			As we can see, about six percent of all named entities end with "an" and about five percent of all non named entities end with "he". In the next step it is important to find out, if some endings are typical for named entities or for non named entities. So we examined the appearances of the most frequent ltc of named entities in tokens that are non named entities and vice versa. The results for the ten most frequent ltc are shown in figure \ref{fig:ten_most_frequent_comparison}. The left side shows the comparison for the most frequent ltc of named entities. The blue curve represents the occurences of the ltc in named entities and the orange curve represents the occurences of the ltc in non named entities. So we can imply, that the greater the distance between the both curves is, the more specific the ltc is for the class. We identified the ltc "an", "ia", and "na" as typical for named entities and analogously the ltc "he", ".", ",", "ed" and "of" as typical for non named entities.\\
			
			\begin{figure}[h]
				\centering
				\includegraphics[scale=0.5]{img/last_two_letters_comparison.png}
				\caption{This figures show the percentages of the ten most frequent ltc in comparison of the classes named entity and non named entity}
				\label{fig:ten_most_frequent_comparison}
			\end{figure}
						
			In a first testrun with this observations, we used only the ltc, that are typical for non named entities (ltcForO). As we can see in table \ref{tab:results}, we do not get an improvement in comparison to the baseline with this new feature. When we add this features to the capital- and the number-of-char-feature, we even decrease the evaluation in comparison to the use of both of the features.\\
			
			We assumed the insufficient classification as the reason for the lower evaluation. We determined just two classes: named entities and non named entities. But there are finer classes for the named entities. So in a second step we searched for the five most frequent ltc for each class and plotted it against the frequencies of the ltc in the other classes (see figure \ref{fig:ltc_for_all_classes}). The cell for a specific ltc in a specific class is brighter, the more typical the ltc is for this class (i.e. the more token in the class end with the ltc). Thus we can use a ltc as a feature, if a cell for this ltc is nearly white or if all other cells in the column of a ltc are black (i.e. this ltc does not occur in any other class).
			
			\begin{figure}[h]
				\centering
				\includegraphics[scale=0.6]{img/ltc_all_classes.png}
				\caption{This figure show the percentages of the five most frequent ltc per classes}
				\label{fig:ltc_for_all_classes}
			\end{figure}
		
		\subsection{POS-Tags}
			With the same strategy as in figure \ref{fig:ltc_for_all_classes} we inspected the POS tags per each class. The relevant POS tags are plotted in figure \ref{fig:pos_per_class}. All other POS tags are not on a significant level except for the class "O".
			
			\begin{figure}[h]
				\centering
				\includegraphics[scale=0.6]{img/pos_per_class.png}
				\caption{Relevant POS tags for each class}
				\label{fig:pos_per_class}
			\end{figure}
		
		\subsection{Chunks}
			As a last feature, we added the chunk annotation provided by the dataset. The relevant chunks are shown in figure \ref{fig:chunks_per_class}. We can see, that the most relevant chunk is "I-INTJ". But we have to consider, that the relative frequency of this chunk is only by 25 \% in all classes. So this chunk rather represents the class "O". 
			
			\begin{figure}[h]
				\centering
				\includegraphics[scale=0.6]{img/chunks_per_class.png}
				\caption{Relevant chunks for each class}
				\label{fig:chunks_per_class}
			\end{figure}
			\newpage
	
	\section{Results of the test set}
		In this section, we present our results of the evaluation on the finally given test set. In this run, we used the same configuration, we described at the end of section \ref{sec:evaluation}. Also we used the training set merged with the dev set as a new training set. This evaluation gives the following values:
		\begin{align}
\text{Micro F1:} &= 0.89022\\
\text{Macro F1:} &= 0.44749
		\end{align}
	
	\newpage
	\appendix
		\section{Results}\label{app:results}\centering
			\rotatebox{90}{\includegraphics[scale=0.58]{img/results_table.png}}
			
\begin{comment}
			\begin{sidewaystable}[htbp]
				\centering
\caption{Results of the evaluation. There is a "1", if the feature was used for the evaluation run. The captions of the first three columns are abbreviations (D = Dummy, FLC = FirstLetterCapital, NrC = NumberOfChars)}
\begin{tabular}{ccccccccccccccccccccccccccc}
\toprule
D & FLC & NrC & er & rs & al & sh & ic & S. & ia & nd & PD & DP & DS & WA & UH & NNSYM & NNPS & NNP & I-NP & I-INTJ & Micro F1 & Macro F1\\ \midrule
1 &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  & 0,8253 & 0,11304\\
 & 1 & 1 &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  & 0,86023 & 0,26933\\
 & 1 & 1 & 1 &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  & 0,860838 & 0,270758\\ 
 & 1 & 1 &  & 1 &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  & 0,860687 & 0,271021\\ 
 & 1 & 1 &  &  & 1 &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  & 0,861979 & 0,274005\\ 
 & 1 & 1 &  &  &  & 1 &  &  &  &  &  &  &  &  &  &  &  &  &  &  & 0,86172 & 0,287434\\ 
 & 1 & 1 &  &  &  &  & 1 &  &  &  &  &  &  &  &  &  &  &  &  &  & 0,859933 & 0,270984\\ 
 & 1 & 1 &  &  &  &  &  & 1 &  &  &  &  &  &  &  &  &  &  &  &  & 0,860622 & 0,269557\\ 
 & 1 & 1 &  &  &  &  &  &  & 1 &  &  &  &  &  &  &  &  &  &  &  & 0,862539 & 0,275036\\ 
 & 1 & 1 &  &  &  &  &  &  &  & 1 &  &  &  &  &  &  &  &  &  &  & 0,861139 & 0,271407\\ 
 & 1 & 1 &  &  &  &  &  &  &  &  & 1 &  &  &  &  &  &  &  &  &  & 0,86045 & 0,269773\\ 
 & 1 & 1 &  &  &  &  &  &  &  &  &  & 1 &  &  &  &  &  &  &  &  & 0,860342 & 0,269396\\ 
 & 1 & 1 &  &  &  &  &  &  &  &  &  &  & 1 &  &  &  &  &  &  &  & 0,860321 & 0,269303\\ 
 & 1 & 1 &  &  &  &  &  &  &  &  &  &  &  & 1 &  &  &  &  &  &  & 0,860385 & 0,269374\\ 
 & 1 & 1 &  &  &  &  &  &  &  &  &  &  &  &  &  & 1 &  &  &  &  & 0,8605146 & 0,2697153\\ 
 & 1 & 1 &  &  &  &  &  &  &  &  &  &  &  &  &  &  & 1 &  &  &  & 0,8604716 & 0,2696812\\ 
 & 1 & 1 &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  & 1 &  &  & 0,8614191 & 0,2759899\\ 
 & 1 & 1 &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  & 1 &  & 0,8657047 & 0,3006439\\ 
 & 1 & 1 &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  & 1 & & 0,8596963 & 0,2688408\\ 
 & 1 & 1 &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  & 1 & 0,8603424 & 0,2693733\\ 
 & 1 & 1 & 1 & 1 & 1 & 1 &  & 1 & 1 & 1 & 1 & 1 &  & 1 &  &  &  &  &  &  & 0,867212 & 0,301131\\ 
 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 &  &  &  &  &  &  & 0,867126 & 0,301915\\ 
 & 1 & 1 &  &  &  &  &  &  &  &  &  &  &  &  & 1 & 1 & 1 & 1 &  &  & 0,868978 & 0,309946\\ 
 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 &  &  & 0,875288 & 0,326584\\ 
 & 1 & 1 &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  & 1 & 1 & 0,859782 & 0,268545\\ 
 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 &  & 1 & 0,87503 & 0,326133\\ \bottomrule
\end{tabular}
\label{tab:results}
\end{sidewaystable}
\end{comment}


\end{document}